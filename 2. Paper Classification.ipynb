{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c825502d-a843-4706-a392-afb7c79f02c4",
   "metadata": {},
   "source": [
    "### 0. 引入基本的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d6e6986-6a0f-4c94-9279-4b0ee52b16bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79b1aec-d0b5-4bcf-bd5a-d0cd5e24a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6837f62-b2f9-4f53-9d27-9ec981aaa349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asce数据128条\n",
    "asce_data = pd.read_csv(\"./data/Asce_Result.csv\", encoding=\"ISO-8859-1\") # encoding=\"ISO-8859-1\"能够表示大部分欧洲的文字\n",
    "# WoS数据207条\n",
    "wos_data = pd.read_csv(\"./data/WoS_Result.csv\", encoding=\"ISO-8859-1\")\n",
    "# Scopus数据247条\n",
    "scopus_data = pd.read_csv(\"./data/Scopus_Result.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6a24535-ffce-4b8d-ad96-127a8260ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_list = []\n",
    "for i in range(len(wos_data)):\n",
    "    paper = {}\n",
    "    paper[\"title\"] = wos_data.iloc[i][\"Article Title\"].lower().replace(\"–\",\"-\").replace(\"’\",\"'\").replace(\"—\",\"-\").replace(\"&nbsp;\", \" \").replace(\"−\", \"-\")\n",
    "    paper[\"source\"] = wos_data.iloc[i][\"Source Title\"].lower()\n",
    "    paper[\"abstract\"] = wos_data.iloc[i][\"Abstract\"].lower().replace(\"–\",\"-\").replace(\"’\",\"'\").replace(\"—\",\"-\").replace(\"&nbsp;\", \" \").replace(\"−\", \"-\")\n",
    "    if wos_data.iloc[i][\"ï»¿publication type\"] == \"J\":\n",
    "        paper[\"type\"] = \"article\"\n",
    "    elif wos_data.iloc[i][\"ï»¿publication type\"] == \"C\":\n",
    "        paper[\"type\"] = \"proceeding\"\n",
    "    paper[\"authors\"] = wos_data.iloc[i][\"Authors\"].replace(\" /\", \"; \")\n",
    "    paper[\"year\"] = wos_data.iloc[i][\"Publication Year\"]\n",
    "    if pd.isnull(wos_data.iloc[i][\"DOI Link\"]) == False:\n",
    "        paper[\"doi\"] = \"https://doi.org/\" + wos_data.iloc[i][\"DOI\"]\n",
    "    else:\n",
    "        paper[\"doi\"] = None\n",
    "    wos_list.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ad101b4-c083-4193-9b4a-f83739407e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_list = []\n",
    "for i in range(len(scopus_data)):\n",
    "    paper = {}\n",
    "    paper[\"title\"] = scopus_data.iloc[i][\"Title\"].lower().replace(\"–\",\"-\").replace(\"’\",\"'\").replace(\"—\",\"-\").replace(\"&nbsp;\", \" \").replace(\"−\", \"-\")\n",
    "    paper[\"source\"] = scopus_data.iloc[i][\"Source title\"].lower()\n",
    "    paper[\"abstract\"] = scopus_data.iloc[i][\"Abstract\"].lower().replace(\"–\",\"-\").replace(\"’\",\"'\").replace(\"—\",\"-\").replace(\"&nbsp;\", \" \").replace(\"−\", \"-\")\n",
    "    \n",
    "    if scopus_data.iloc[i][\"Document Type\"] == \"Article\":\n",
    "        paper[\"type\"] = \"article\"\n",
    "    else:\n",
    "        paper[\"type\"] = \"proceeding\"\n",
    "\n",
    "    paper[\"authors\"] = scopus_data.iloc[i][\"ï»¿authors\"].replace(\" /\", \"; \")\n",
    "    paper[\"year\"] = scopus_data.iloc[i][\"Year\"]\n",
    "    \n",
    "    if pd.isnull(scopus_data.iloc[i][\"DOI\"]) == False:\n",
    "        paper[\"doi\"] = \"https://doi.org/\" + scopus_data.iloc[i][\"DOI\"]\n",
    "    else:\n",
    "        paper[\"doi\"] = None\n",
    "    scopus_list.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e48115c-6425-4ed8-8b2d-84b32501fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "asce_list = []\n",
    "for i in range(len(asce_data)):\n",
    "    paper = {}\n",
    "    paper[\"title\"] = asce_data.iloc[i][\"title\"].lower().replace(\"–\",\"-\").replace(\"’\",\"'\").replace(\"—\",\"-\").replace(\"&nbsp;\", \" \").replace(\"−\", \"-\")\n",
    "    paper[\"source\"] = asce_data.iloc[i][\"source\"].lower()\n",
    "    paper[\"abstract\"] = asce_data.iloc[i][\"abstract\"].lower().replace(\"–\",\"-\").replace(\"’\",\"'\").replace(\"—\",\"-\").replace(\"&nbsp;\", \" \").replace(\"−\", \"-\")\n",
    "    \n",
    "    if asce_data.iloc[i][\"type\"] == \"Journal Article\":\n",
    "        paper[\"type\"] = \"article\"\n",
    "    else:\n",
    "        paper[\"type\"] = \"proceeding\"\n",
    "\n",
    "    paper[\"authors\"] = asce_data.iloc[i][\"Authors\"][2:].replace(\" /\", \"; \")\n",
    "    paper[\"year\"] = asce_data.iloc[i][\"pubyear\"]\n",
    "    \n",
    "    if pd.isnull(asce_data.iloc[i][\"doi\"]) == False:\n",
    "        paper[\"doi\"] = asce_data.iloc[i][\"doi\"]\n",
    "    else:\n",
    "        paper[\"doi\"] = None\n",
    "    asce_list.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94496106-eba8-4824-adfe-b26a471bfa89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc10c212-4adc-425a-a61d-a6aa803b561a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9471dcd2-83ab-4c61-a0e1-44cf1f78f5f3",
   "metadata": {},
   "source": [
    "### 1. 文件读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b9d8537-4bae-4cfc-bf30-9971bfeca9bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\deepBasedReview\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ee340dd-a177-40a7-8826-1a3f3f08c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings(sentence, tokenizer, model,device):\n",
    "    tokens = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens[\"input_ids\"])\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee604005-1813-45f7-b312-e91f529a0e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/206 [00:00<?, ?it/s]E:\\anaconda\\envs\\deepBasedReview\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████████████████████████████████████████████████████████| 206/206 [00:15<00:00, 13.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 15.714799404144287s\n",
      "average time: 0.07628543400070043s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wos_list_ebds = []\n",
    "wos_time_start = time.time()\n",
    "for i in tqdm(range(len(wos_list))):\n",
    "    paper = wos_list[i]\n",
    "    title = paper[\"title\"]\n",
    "    abstract = paper[\"abstract\"]\n",
    "    title_ebds = getEmbeddings(title, tokenizer, model, device)\n",
    "    abstract_ebds = getEmbeddings(abstract[:100], tokenizer, model, device)\n",
    "    paper[\"title_ebds\"] = title_ebds\n",
    "    paper[\"abstract_ebds\"] = abstract_ebds\n",
    "    paper[\"duplicated\"] = 0\n",
    "    wos_list_ebds.append(paper)\n",
    "wos_time_end = time.time()\n",
    "print(f\"total time: {wos_time_end - wos_time_start}s\")\n",
    "print(f\"average time: {(wos_time_end - wos_time_start)/len(wos_list)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f05baec5-6fde-4e1f-ae2a-7fae98286196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 247/247 [00:23<00:00, 10.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 23.195196866989136s\n",
      "average time: 0.0939076796234378s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scopus_list_ebds = []\n",
    "scopus_time_start = time.time()\n",
    "for i in tqdm(range(len(scopus_list))):\n",
    "    paper = scopus_list[i]\n",
    "    title = paper[\"title\"]\n",
    "    abstract = paper[\"abstract\"]\n",
    "    title_ebds = getEmbeddings(title, tokenizer, model, device)\n",
    "    abstract_ebds = getEmbeddings(abstract[:100], tokenizer, model, device)\n",
    "    paper[\"title_ebds\"] = title_ebds\n",
    "    paper[\"abstract_ebds\"] = abstract_ebds\n",
    "    paper[\"duplicated\"] = 0\n",
    "    scopus_list_ebds.append(paper)\n",
    "scopus_time_end = time.time()\n",
    "print(f\"total time: {scopus_time_end - scopus_time_start}s\")\n",
    "print(f\"average time: {(scopus_time_end - scopus_time_start)/len(scopus_list)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a804967-4390-49f0-b2dc-aede3062be35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 128/128 [00:14<00:00,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 14.32786250114441s\n",
      "average time: 0.1119364257901907s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "asce_list_ebds = []\n",
    "asce_time_start = time.time()\n",
    "for i in tqdm(range(len(asce_list))):\n",
    "    paper = asce_list[i]\n",
    "    title = paper[\"title\"]\n",
    "    abstract = paper[\"abstract\"]\n",
    "    title_ebds = getEmbeddings(title, tokenizer, model, device)\n",
    "    abstract_ebds = getEmbeddings(abstract[:100], tokenizer, model, device)\n",
    "    paper[\"title_ebds\"] = title_ebds\n",
    "    paper[\"abstract_ebds\"] = abstract_ebds\n",
    "    paper[\"duplicated\"] = 0\n",
    "    asce_list_ebds.append(paper)\n",
    "asce_time_end = time.time()\n",
    "print(f\"total time: {asce_time_end - asce_time_start}s\")\n",
    "print(f\"average time: {(asce_time_end - asce_time_start)/len(asce_list)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f88ff30c-fc1d-4951-bc91-3909ecf22ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_list_ebds_backup = copy.deepcopy(wos_list_ebds)\n",
    "scopus_list_ebds_backup = copy.deepcopy(scopus_list_ebds)\n",
    "asce_list_ebds_backup = copy.deepcopy(asce_list_ebds)\n",
    "\n",
    "wos_list_ebds = wos_list_ebds_backup.copy()\n",
    "scopus_list_ebds = scopus_list_ebds_backup.copy()\n",
    "asce_list_ebds = asce_list_ebds_backup.copy()\n",
    "\n",
    "combined_list_ebds = []\n",
    "combined_list_ebds.extend(wos_list_ebds)\n",
    "combined_list_ebds.extend(scopus_list_ebds)\n",
    "combined_list_ebds.extend(asce_list_ebds)\n",
    "combined_list_ebds = sorted(combined_list_ebds, key=lambda x:x[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1550a1bb-e447-4a96-8cc2-f613678da706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing time: 0.4702026844024658s\n",
      "avarage processing time: 0.0008092989404517484\n",
      "total item: 581\n",
      "total filtered item: 401\n"
     ]
    }
   ],
   "source": [
    "deleteList = []\n",
    "start_time = time.time()\n",
    "for i in range(0, len(combined_list_ebds)):\n",
    "    if i in deleteList:\n",
    "        continue\n",
    "    paper_i = combined_list_ebds[i]\n",
    "    paper_i_title_ebds = paper_i[\"title_ebds\"]\n",
    "\n",
    "    for j in range(i+1, len(combined_list_ebds)):\n",
    "        paper_j = combined_list_ebds[j]\n",
    "        paper_j_title_ebds = paper_j[\"title_ebds\"]\n",
    "        title_similarity = cosine_similarity(paper_i_title_ebds, paper_j_title_ebds)\n",
    "        if title_similarity > 0.95:\n",
    "            deleteList.append(j)\n",
    "            \n",
    "            if len(paper_i[\"abstract\"]) >= len(paper_j[\"abstract\"]):\n",
    "                paper_i[\"abstract\"] = paper_i[\"abstract\"]\n",
    "            else:\n",
    "                paper_i[\"abstract\"] = paper_j[\"abstract\"]\n",
    "                paper_i[\"abstract_ebds\"] = paper_j[\"abstract_ebds\"]\n",
    "                \n",
    "            paper_i[\"authors\"] = paper_i[\"authors\"] if len(paper_i[\"authors\"]) >= len(paper_j[\"authors\"]) else paper_j[\"authors\"]\n",
    "            \n",
    "            if paper_i[\"doi\"] == None and paper_j[\"doi\"] != None:\n",
    "                paper_i[\"doi\"] = paper_j[\"doi\"]\n",
    "                \n",
    "            paper_j[\"duplicated\"] = 1\n",
    "        else:\n",
    "            break\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"processing time: {end_time - start_time}s\")\n",
    "print(f\"avarage processing time: {(end_time - start_time) / len(combined_list_ebds)}\")\n",
    "print(f\"total item: {len(combined_list_ebds)}\")\n",
    "print(f\"total filtered item: {len(combined_list_ebds) - len(deleteList)}\")\n",
    "\n",
    "# 注1：后续可以通过paper[\"duplicated\"] == 0 获取unique paper。如下面代码所示：\n",
    "# for paper in combined_list_ebds:\n",
    "#     if paper[\"duplicated\"] == 0:\n",
    "#         print(paper[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a61897f2-f506-4d3b-b0fb-7c11d26bc5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 将数据combined_list_ebds存到本地\n",
    "# with open(\"./results/results_2.pkl\", \"wb\") as fp:\n",
    "#     pickle.dump(combined_list_ebds, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "09d8e71f-4d2d-43a5-a361-ccc6efd749ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 将数据combined_list_ebds加载内存\n",
    "with open(\"./results/results_2.pkl\", \"rb\") as fp:\n",
    "    combined_list_ebds = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3730b582-c072-4cc3-868d-b180c2886abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"year\", \"title\", \"abstract\"]\n",
    "papers_df = pd.DataFrame(columns = columns)\n",
    "for i, paper in enumerate(combined_list_ebds):\n",
    "    if paper[\"duplicated\"] == 0:\n",
    "        paperList = pd.DataFrame([{\n",
    "            \"year\":paper[\"year\"],\n",
    "            \"title\":paper[\"title\"],\n",
    "            \"abstract\":paper[\"abstract\"]\n",
    "        }])\n",
    "        papers_df = pd.concat([papers_df, paperList], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b9646fa-9768-4882-ad8f-99491270f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 将数据(title abstract)写到本地\n",
    "# papers_df.to_csv(\"./results/results_2.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "99ee4ec9-13ef-4527-afcd-01daab42b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 获取测试数据(将数据(title abstract)加载到内存)\n",
    "papers_df = pd.read_csv(\"./results/results_2.csv\", index_col=\"num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3019be37-7377-4d33-8e68-de602c350979",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_gt_df = pd.read_csv(\"./results/results_gt_2.csv\", index_col=\"num\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031ddaff-d69d-4aa8-8024-261fbb66370d",
   "metadata": {},
   "source": [
    "#### 1.1 设置随机种子&提取测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f52d9971-1ebc-47c5-9a63-aba160aa6810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5a54948d-70de-4c62-85ed-e551a9135729",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indexes = np.arange(len(papers_df))\n",
    "selected_indexes = np.array([])\n",
    "\n",
    "set_seed(42)\n",
    "indexes_1 = np.sort(np.random.choice(\n",
    "    len(papers_df)\n",
    "    , 100,  replace=False))\n",
    "selected_indexes = np.concatenate((selected_indexes, indexes_1), axis=0)\n",
    "\n",
    "set_seed(42)\n",
    "indexes_2 = np.sort(np.random.choice(\n",
    "    all_indexes[~np.isin(all_indexes, selected_indexes)]\n",
    "    , 100, replace=False))\n",
    "selected_indexes = np.concatenate((selected_indexes, indexes_2), axis=0)\n",
    "\n",
    "set_seed(42)\n",
    "indexes_3 = np.sort(np.random.choice(\n",
    "    all_indexes[~np.isin(all_indexes, selected_indexes)]\n",
    "    , 100, replace=False))\n",
    "selected_indexes = np.concatenate((selected_indexes, indexes_3), axis=0)\n",
    "\n",
    "indexes_4 = all_indexes[~np.isin(all_indexes, selected_indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d7ba11b3-2b69-406b-96c8-e34c53e9bb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indexes_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "85ee2f46-ac08-411d-adca-fd103e25a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "defined_indexes =  [4,169,378]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "fd6b1354-6e8c-44cc-9f4c-b5b24b0f47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = papers_df.iloc[indexes_4].copy()\n",
    "gt_data = papers_gt_df.iloc[indexes_4].copy()\n",
    "\n",
    "test_data[\"groundTruth\"] = gt_data[\"type\"]\n",
    "test_data[\"prediction\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d3b72b-4f07-4fe5-85ef-d17907aac099",
   "metadata": {},
   "source": [
    "### 2. LLM进行相关性判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce0c140f-279d-417d-83c7-a492b0fcf3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "402e212d-48dd-4931-823a-e544b3bb9115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab8a4e-b139-4451-abb9-baa208c65c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "bbc4bb9e-5c0d-4e1d-aec7-f2ed7209be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = \"\"\"\n",
    "    Task: \n",
    "    You are required to perform a text classification task over the provided materials.\n",
    "    You will be provided with the title and abstract of an research paper. \n",
    "    Based on the title and abstract, you are required to determine whether the paper meets all the following requirements： \n",
    "        Requirement 1. The research falls in the construction domain or construction site management domain.\n",
    "        \n",
    "        Requirement 2. The research uses visual materials (limited to video or image sequences) or point cloud as data.\n",
    "        \n",
    "        Requirement 3. The paper has applied object tracking method or object tracking algorithm in the research. The topic can be any one of the followings:\n",
    "                a. \"applying computer vision and/or machine learning to achieve object or worker or equipment tracking in construction domain\" or \\\n",
    "                b. \"using object tracking algorithm to solve part of the research problem\" or \\\n",
    "                c. \"investigating 2-dimensional and/or 3-dimensional object tracking in construction domain\" or \\\n",
    "                d. \"improving or optimizing object tracking algorithm in construction domain\" or \\\n",
    "                e. \"solving research problem using object tracking method to track object or worker or equipment\" or \\\n",
    "                f. \"supervising construction site using techniques including object tracking techniques or object tracking system\".\n",
    "                       \n",
    "    For the research paper meets all three requirement, the paper should be classified as type 1;\n",
    "    For the research paper does not meet any one of the requirements, the paper should be classified as type 2.\n",
    "    \n",
    "    Instructions:\n",
    "    1. For paper using LiDAR, UWB, GPS, RFID, bluetooth, IOT, GIS, ZIGBEE, AR or other non-image and non-video sensors to achieve \\\n",
    "    object or worker tracking, it does not meet the requirement 2. Therefore, this paper should be categorized as type 2.\n",
    "\n",
    "    2. If the research paper only focuses on monitoring or object detecting/detection or locating/localization or positioning or matching, without applying object tracking algorithm, \\\n",
    "    the paper is out of the scope of this task, as it does not meet the Requirement 3. Therefore, this paper should be categorized as type 2.\n",
    "    If the research paper focuses on monitoring or object detecting/detection or locating/localization or positioning or matching, \\\n",
    "    at the same time, the paper explicitly mentioned using object tracking algorithm or object tracking method or tracking object. The paper is within the scope of this task.\n",
    "    \n",
    "    3. If a paper applies computer vision and/or machine learning, but not to achieve tracking function, it is out of the scope of this task.\n",
    "\n",
    "    4. If a paper used vision-based algorithm, then the paper meets the requirement 2.\n",
    "\n",
    "    5. If the title and abstract do not explicitly mention using \"object tracking method\" or mention \"tracking object or worker\", \\\n",
    "    then the paper is out of the scope of this task, as it does not meet requirement 3.\n",
    "\n",
    "    6. Additional Information:\n",
    "    (1) For paper related to \"crane\", it is out of the scope of this task. Therefore, the paper focusing on crane tracking should be categorized as type 2.\n",
    "    (2) Domain like nuclear power plant is out of the scope of this task.\n",
    "    (3) The application of SLAM technology is out of the scope of this task.\n",
    "    (4) electric power monitor is out of the scope of this task.\n",
    "    (5) tracking construction progress and tracking construction project are out of the scope of this task.\n",
    "    (6) obtaining trajectory of the detected object and tracking object's movement are also regarded as tracking object.\n",
    "    (7) The research outcomes is expected to be useful for tracking, then the paper is not related to applying tracking algorithm. Such paper is out \\\n",
    "    of the scope of this task.\n",
    "    (8) automatic tracking camera system or camera control system is out of the scope of this task\n",
    "\n",
    "    \n",
    "    7. Please output the answer in JSON format. \n",
    "    {{\n",
    "        \"Title\": \"the title of the provided paper\",\n",
    "        \"Type\" : \"the type of the provided paper\",\n",
    "        \"RequirementsFulfilled\": {{\n",
    "            \"Requirement_1\": \"Whether the paper meets requirement 1. Value should be true or false\",\n",
    "            \"Requirement_2\": \"Whether the paper meets requirement 2. Value should be true or false\",\n",
    "            \"Requirement_3\": \"Whether the paper meets requirement 3. Value should be true or false\",\n",
    "            \"Reason\": \"When the requirement 3 is true, you should give a reason why, and provided reference from title or abstract\"\n",
    "        }}\n",
    "    }}\n",
    "    \n",
    "    8. Here is an example:\n",
    "    Title:\n",
    "        Personnel tracking on construction sites using video cameras\n",
    "    Abstract:\n",
    "        This paper discusses the possibility of- and need for-tracking workforce on construction jobsites using video cameras. \\\n",
    "    An evaluation of algorithms and their associated results is presented. The principal objective of this paper is to test and \\\n",
    "    demonstrate the feasibility of tracking workers from statically placed and dynamically moving cameras. This paper also reviews \\\n",
    "    existing techniques to monitor workforce and describes areas where this work might be useful in engineering applications. \\\n",
    "    The main difficulties associated with tracking on a construction site is the significant amount of visual clutter, the changing \\\n",
    "    photometric visual content throughout the course of a day, and the presence of occluding and moving obstacles. The tracking of workers \\\n",
    "    within the field of view of the camera will involve four tracking techniques, density mean-shift, Bayesian segmentation, active contours, \\\n",
    "    and graph-cuts. Typical construction site video will be processed using the proposed algorithms and analyzed to determine the most appropriate \\\n",
    "    tracking method for the video presented.\n",
    "    \n",
    "    Answer:\n",
    "    {{\n",
    "        \"Title\": \"Real-time positioning of moving objects by dynamic target tracking\",\n",
    "        \"Type\": \"1\",\n",
    "        \"RequirementsFulfilled\": {{\n",
    "            \"Requirement_1\": true,\n",
    "            \"Requirement_2\": true,\n",
    "            \"Requirement_3\": true,\n",
    "            \"Reason\": \"The abstract states that 'the objective of this paper is to test and demonstrate the feasibility of tracking workers from cameras', which meets all three requirements\"\n",
    "        }}\n",
    "    }}\n",
    "    \n",
    "    Notes:\n",
    "    1. Please think carefully and make sure that you get the correct answer.\n",
    "    2. Do not include any explanations or apologies in your responses.\n",
    "    3. Do not respond to any questions that might ask anything else than output the answer in JSON format.\n",
    "    4. Please do not imply anything, answer the question based on the provided title, abstract, and Instruction.\n",
    "\n",
    "    Title:\n",
    "    {title}\n",
    "    Abstract:\n",
    "    {abstract}\n",
    "\n",
    "    Now please complete the task:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt1 = PromptTemplate.from_template(template=template1)\n",
    "# chatllm1 = ChatOpenAI(model = \"gpt-4o-mini-2024-07-18\") #gpt-4o-mini-2024-07-18 # gpt-3.5-turbo-1106\n",
    "# lcel1 = prompt1 | chatllm1 | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6750f2-222b-4c7c-962b-595eb1fe4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) The using of robot or robotics is out of the scope of this task.\n",
    "# \"Reason\": \"The abstract states that the 'objective of this paper is to test and demonstrate the feasibility of tracking workers from cameras', which meets all three requirements\"\n",
    "# \"Reason\": \"When the requirement 3 is true, you should give a reason why\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "dc0244b3-507b-4f44-b1fa-d741d0a1e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 15\n",
    "result = lcel1.invoke({\"title\": test_data.loc[id][\"title\"], \"abstract\": test_data.loc[id][\"abstract\"]})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2dabc2-7261-43ce-b2f3-eda69da5bbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "141723ee-974b-4f1f-888c-ed33327aa079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [07:48,  4.64s/it]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "count = 0\n",
    "for index, paper in tqdm(test_data.iterrows()):\n",
    "    \n",
    "    chatllm1 = ChatOpenAI(model = \"gpt-4o-mini-2024-07-18\") #gpt-4o-mini-2024-07-18 # gpt-3.5-turbo-1106\n",
    "    lcel1 = prompt1 | chatllm1 | JsonOutputParser()\n",
    "\n",
    "    title = paper[\"title\"]\n",
    "    abstract = paper[\"abstract\"]\n",
    "    result = lcel1.invoke({\"title\": title, \"abstract\": abstract})\n",
    "    results[index] = result\n",
    "    test_data.loc[index, \"prediction\"] = int(result[\"Type\"])\n",
    "    # count+=1\n",
    "    # if count <= 50:\n",
    "    #     continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "1a3ed7c7-f212-4165-a15f-62f9966aa90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/results_code2/result_5.json\", \"w\") as fp:\n",
    "    fp.write(json.dumps(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "535b2781-e61b-492d-8ef3-e5121ff48254",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(\"./results/results_code2/results_pd_5.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830aed8a-fb49-4451-9738-7ac1ac6034dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "10e0ec5f-96e7-4dc8-a6fd-e2b4b63efbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: {'Title': 'computer vision-based video interpretation model for automated productivity analysis of construction operations',\n",
       "  'Type': '2',\n",
       "  'RequirementsFulfilled': {'Requirement_1': True,\n",
       "   'Requirement_2': True,\n",
       "   'Requirement_3': False,\n",
       "   'Reason': \"The abstract discusses 'detection and tracking of project resources', but does not explicitly mention using 'object tracking method' or 'tracking object or worker', which does not meet requirement 3.\"}},\n",
       " 169: {'Title': 'predicting safety hazards among construction workers and equipment using computer vision and deep learning techniques',\n",
       "  'Type': '2',\n",
       "  'RequirementsFulfilled': {'Requirement_1': True,\n",
       "   'Requirement_2': True,\n",
       "   'Requirement_3': False,\n",
       "   'Reason': 'The paper discusses monitoring and detecting locations and trajectories of workers and equipment, but it does not explicitly mention using an object tracking method or algorithm.'}},\n",
       " 378: {'Title': 'efficient project management in construction sites to monitor and track the employees using multi-modal deep learning model',\n",
       "  'Type': '2',\n",
       "  'RequirementsFulfilled': {'Requirement_1': True,\n",
       "   'Requirement_2': False,\n",
       "   'Requirement_3': False,\n",
       "   'Reason': 'The paper discusses monitoring and tracking employees but does not explicitly mention using visual materials or object tracking methods as per the requirements.'}}}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7fb6c-a98e-4e81-907f-1d26325f5b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0871b24c-f586-4578-9ed2-310f60417d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b6d9b-b877-40d3-a5da-fba1de6cebd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc803906-39d7-4084-a666-bcaa589aa85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea4d76-d381-4b13-9f73-196ba5e9ee3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cec388-69ae-4473-89c1-b345b4c3a71c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c71a5f-5fca-4540-bed3-8d93a829c178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
