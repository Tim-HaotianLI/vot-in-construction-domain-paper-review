{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b87fa68-b73d-4b55-bf9d-6d518c54154b",
   "metadata": {},
   "source": [
    "## 1. 加载数据和整理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a6684d7-64f6-4898-b599-f3d2506a9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd88100f-4a33-402e-8b9a-2bd0105ff0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asce数据128条\n",
    "asce_data = pd.read_csv(\"./data/Asce_2_Result.csv\", encoding=\"utf-8\") # encoding=\"ISO-8859-1\"能够表示大部分欧洲的文字\n",
    "# WoS数据207条\n",
    "wos_data = pd.read_csv(\"./data/WoS_2_Result.csv\", encoding=\"utf-8\")\n",
    "# Scopus数据247条\n",
    "scopus_data = pd.read_csv(\"./data/Scopus_2_Result.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ecbbd92e-1589-472e-a513-f63e5bd40513",
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_list = []\n",
    "for i in range(len(wos_data)):\n",
    "    paper = {}\n",
    "    paper[\"title\"] = wos_data.iloc[i][\"Article Title\"].lower().replace(\"–\",\"-\").replace(\"’\",\"'\").replace(\"—\",\"-\").replace(\"&nbsp;\", \" \").replace(\"−\", \"-\")\n",
    "    paper[\"source\"] = wos_data.iloc[i][\"Source Title\"].lower()\n",
    "    paper[\"abstract\"] = wos_data.iloc[i][\"Abstract\"].lower().replace(\"–\",\"-\").replace(\"’\",\"'\").replace(\"—\",\"-\").replace(\"&nbsp;\", \" \").replace(\"−\", \"-\")\n",
    "    if wos_data.iloc[i][\"publication type\"] == \"J\":\n",
    "        paper[\"type\"] = \"article\"\n",
    "    elif wos_data.iloc[i][\"publication type\"] == \"C\":\n",
    "        paper[\"type\"] = \"proceeding\"\n",
    "    paper[\"authors\"] = wos_data.iloc[i][\"Authors\"].replace(\" /\", \"; \")\n",
    "    paper[\"year\"] = wos_data.iloc[i][\"Publication Year\"]\n",
    "    if pd.isnull(wos_data.iloc[i][\"DOI Link\"]) == False:\n",
    "        paper[\"doi\"] = \"https://doi.org/\" + wos_data.iloc[i][\"DOI\"]\n",
    "    else:\n",
    "        paper[\"doi\"] = None\n",
    "    wos_list.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be917850-027c-41aa-bc95-5ad5564b6436",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_list = []\n",
    "for i in range(len(scopus_data)):\n",
    "    paper = {}\n",
    "    paper[\"title\"] = scopus_data.iloc[i][\"Title\"].lower().replace(\"–\",\"-\").replace(\"’\",\"'\").replace(\"—\",\"-\").replace(\"&nbsp;\", \" \").replace(\"−\", \"-\")\n",
    "    paper[\"source\"] = scopus_data.iloc[i][\"Source title\"].lower()\n",
    "    paper[\"abstract\"] = scopus_data.iloc[i][\"Abstract\"].lower().replace(\"–\",\"-\").replace(\"’\",\"'\").replace(\"—\",\"-\").replace(\"&nbsp;\", \" \").replace(\"−\", \"-\")\n",
    "    \n",
    "    if scopus_data.iloc[i][\"Document Type\"] == \"Article\":\n",
    "        paper[\"type\"] = \"article\"\n",
    "    else:\n",
    "        paper[\"type\"] = \"proceeding\"\n",
    "\n",
    "    paper[\"authors\"] = scopus_data.iloc[i][\"authors\"].replace(\" /\", \"; \")\n",
    "    paper[\"year\"] = scopus_data.iloc[i][\"Year\"]\n",
    "    \n",
    "    if pd.isnull(scopus_data.iloc[i][\"DOI\"]) == False:\n",
    "        paper[\"doi\"] = \"https://doi.org/\" + scopus_data.iloc[i][\"DOI\"]\n",
    "    else:\n",
    "        paper[\"doi\"] = None\n",
    "    scopus_list.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67b99c0d-771c-4295-8aba-9f58ff8da3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "asce_data[\"Authors\"] = asce_data[\"Authors\"].apply(lambda x:x.replace(\"\\n\",\", \").replace(\"\\xa0\",\" \").replace(\" and \", \", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c113f564-62c4-452c-8fcc-189c73eb6c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yongchao Yang, Shunlong Li, Satish Nagarajaiah, Hui Li, Peng Zhou'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asce_data.loc[144][\"Authors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "79227b40-850a-4300-a016-1a72bbd16cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "asce_data.to_csv(\"./data/Asce_2_Result_modified.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "86b3dd54-96ac-43c4-b674-4129d854a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "asce_list = []\n",
    "for i in range(len(asce_data)):\n",
    "    paper = {}\n",
    "    paper[\"title\"] = asce_data.iloc[i][\"title\"].lower().replace(\"–\",\"-\").replace(\"’\",\"'\").replace(\"—\",\"-\").replace(\"&nbsp;\", \" \").replace(\"−\", \"-\")\n",
    "    paper[\"source\"] = asce_data.iloc[i][\"source\"].lower()\n",
    "    paper[\"abstract\"] = asce_data.iloc[i][\"abstract\"].lower().replace(\"–\",\"-\").replace(\"’\",\"'\").replace(\"—\",\"-\").replace(\"&nbsp;\", \" \").replace(\"−\", \"-\")\n",
    "    \n",
    "    if asce_data.iloc[i][\"type\"] == \"Journal Article\":\n",
    "        paper[\"type\"] = \"article\"\n",
    "    else:\n",
    "        paper[\"type\"] = \"proceeding\"\n",
    "\n",
    "    paper[\"authors\"] = asce_data.iloc[i][\"Authors\"][2:].replace(\" /\", \"; \")\n",
    "    paper[\"year\"] = asce_data.iloc[i][\"pubyear\"]\n",
    "    \n",
    "    if pd.isnull(asce_data.iloc[i][\"doi\"]) == False:\n",
    "        paper[\"doi\"] = asce_data.iloc[i][\"doi\"]\n",
    "    else:\n",
    "        paper[\"doi\"] = None\n",
    "    asce_list.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9be90bf2-35a5-475a-aece-c46ed0a66e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'annotating 2d imagery with 3d kinematically configurable assets of construction equipment for training pose-informed activity analysis and safety monitoring algorithms',\n",
       " 'source': 'computing in civil engineering 2019: visualization, information modeling, and simulation',\n",
       " 'abstract': \"the availability of inexpensive and high-quality cameras has enabled research towards computer vision systems for tracking construction productivity and safety by detecting, tracking, and estimating pose of construction resources and recognizing their activities. to verify these algorithms can generalize to novel visual scenes before deployment, large amounts of labelled training and real-world validation samples are needed. while automatic generation of synthetic data helps with the former, obtaining the latter is less practical. to address this gap, we introduce a tool with which designated annotators perform pose annotation by interactively aligning 3d kinematically configurable equipment models with their depictions on 2d images. multiple types of informative visual annotation can be retrieved using this technique, notably segmentation masks and equipment pose. experiments demonstrate our tool's effectiveness for annotating long-form videos of earthmoving operations. we demonstrate how our tool can provide ground-truth annotations for the evaluation of a variety of computer-vision algorithms.\",\n",
       " 'type': 'proceeding',\n",
       " 'authors': 'minic Roberts, Yunpeng Wang, Ali Sabet, Mani Golparvar-Fard',\n",
       " 'year': 2019,\n",
       " 'doi': 'https://doi.org/10.1061/9780784482421.005'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asce_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dae22f-ba64-4a72-a5e6-b4a5bbc2e646",
   "metadata": {},
   "source": [
    "## 2. 对title和abstract进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6e7dffc-703e-458a-b0b2-97a87ed3d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da79d25d-ff2a-4523-8c1a-ee9df1b3d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "472880d2-4a5a-4c96-b8aa-ed62b83b2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed) # 为cpu设置种子\n",
    "    torch.cuda.manual_seed_all(seed)# 为所有GPU设置种子\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f461bc8-35b5-451f-a5bd-dead331c998b",
   "metadata": {},
   "source": [
    "### 2.1 用BERT进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3026841b-b031-4cd2-8752-1c9d16607e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c80460a-4c22-43be-a989-4f6b54139c86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b43d96c86e146528e5e8d8f7e30927a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software_anaconda\\envs\\LLM\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HKU-i5-Oscar\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168b2b9802fe496497440250ccdcc62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56631c07d644b21af57f7d7a0fadf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67b4f1374dc47229ab8c3395adc0797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de68a2bfc82b4bc59a11d6f9cba61089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89c68b74-af0b-4fb6-bb9b-ed99209a1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings(sentence, tokenizer, model,device):\n",
    "    tokens = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens[\"input_ids\"])\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0064455b-c010-4f09-9fc0-4cfd337af52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 239/239 [00:11<00:00, 21.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 11.292421340942383s\n",
      "average time: 0.04724862485749951s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wos_list_ebds = []\n",
    "wos_time_start = time.time()\n",
    "for i in tqdm(range(len(wos_list))):\n",
    "    paper = wos_list[i]\n",
    "    title = paper[\"title\"]\n",
    "    abstract = paper[\"abstract\"]\n",
    "    title_ebds = getEmbeddings(title, tokenizer, model, device)\n",
    "    abstract_ebds = getEmbeddings(abstract[:100], tokenizer, model, device)\n",
    "    paper[\"title_ebds\"] = title_ebds\n",
    "    paper[\"abstract_ebds\"] = abstract_ebds\n",
    "    paper[\"duplicated\"] = 0\n",
    "    wos_list_ebds.append(paper)\n",
    "wos_time_end = time.time()\n",
    "print(f\"total time: {wos_time_end - wos_time_start}s\")\n",
    "print(f\"average time: {(wos_time_end - wos_time_start)/len(wos_list)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "eba12e5f-e37a-4f88-b288-9fd712daeefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 329/329 [00:16<00:00, 20.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 16.245072603225708s\n",
      "average time: 0.049377120374546224s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scopus_list_ebds = []\n",
    "scopus_time_start = time.time()\n",
    "for i in tqdm(range(len(scopus_list))):\n",
    "    paper = scopus_list[i]\n",
    "    title = paper[\"title\"]\n",
    "    abstract = paper[\"abstract\"]\n",
    "    title_ebds = getEmbeddings(title, tokenizer, model, device)\n",
    "    abstract_ebds = getEmbeddings(abstract[:100], tokenizer, model, device)\n",
    "    paper[\"title_ebds\"] = title_ebds\n",
    "    paper[\"abstract_ebds\"] = abstract_ebds\n",
    "    paper[\"duplicated\"] = 0\n",
    "    scopus_list_ebds.append(paper)\n",
    "scopus_time_end = time.time()\n",
    "print(f\"total time: {scopus_time_end - scopus_time_start}s\")\n",
    "print(f\"average time: {(scopus_time_end - scopus_time_start)/len(scopus_list)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "91030d4b-dc5c-4945-bdb6-fa00fe0098c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 145/145 [00:07<00:00, 19.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 7.507044076919556s\n",
      "average time: 0.051772717771859s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "asce_list_ebds = []\n",
    "asce_time_start = time.time()\n",
    "for i in tqdm(range(len(asce_list))):\n",
    "    paper = asce_list[i]\n",
    "    title = paper[\"title\"]\n",
    "    abstract = paper[\"abstract\"]\n",
    "    title_ebds = getEmbeddings(title, tokenizer, model, device)\n",
    "    abstract_ebds = getEmbeddings(abstract[:100], tokenizer, model, device)\n",
    "    paper[\"title_ebds\"] = title_ebds\n",
    "    paper[\"abstract_ebds\"] = abstract_ebds\n",
    "    paper[\"duplicated\"] = 0\n",
    "    asce_list_ebds.append(paper)\n",
    "asce_time_end = time.time()\n",
    "print(f\"total time: {asce_time_end - asce_time_start}s\")\n",
    "print(f\"average time: {(asce_time_end - asce_time_start)/len(asce_list)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "46c994a8-4b2b-4c16-8b6a-35cf3f297e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_list_ebds_backup = copy.deepcopy(wos_list_ebds)\n",
    "scopus_list_ebds_backup = copy.deepcopy(scopus_list_ebds)\n",
    "asce_list_ebds_backup = copy.deepcopy(asce_list_ebds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5e87e302-0905-438e-9d81-101b247d6296",
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_list_ebds = wos_list_ebds_backup.copy()\n",
    "scopus_list_ebds = scopus_list_ebds_backup.copy()\n",
    "asce_list_ebds = asce_list_ebds_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9906eea2-f7a8-4137-9865-a93085ab2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_list_ebds = []\n",
    "combined_list_ebds.extend(wos_list_ebds)\n",
    "combined_list_ebds.extend(scopus_list_ebds)\n",
    "combined_list_ebds.extend(asce_list_ebds)\n",
    "combined_list_ebds = sorted(combined_list_ebds, key=lambda x:x[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1dc82e8d-571b-4ac4-9593-aed727143a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing time: 0.5564508438110352s\n",
      "avarage processing time: 0.0007804359660743831\n",
      "total item: 713\n",
      "total filtered item: 496\n"
     ]
    }
   ],
   "source": [
    "deleteList = []\n",
    "start_time = time.time()\n",
    "for i in range(0, len(combined_list_ebds)):\n",
    "    if i in deleteList:\n",
    "        continue\n",
    "    paper_i = combined_list_ebds[i]\n",
    "    paper_i_title_ebds = paper_i[\"title_ebds\"]\n",
    "\n",
    "    for j in range(i+1, len(combined_list_ebds)):\n",
    "        paper_j = combined_list_ebds[j]\n",
    "        paper_j_title_ebds = paper_j[\"title_ebds\"]\n",
    "        title_similarity = cosine_similarity(paper_i_title_ebds, paper_j_title_ebds)\n",
    "        if title_similarity > 0.95:\n",
    "            deleteList.append(j)\n",
    "            \n",
    "            if len(paper_i[\"abstract\"]) <= len(paper_j[\"abstract\"]):\n",
    "                paper_i[\"abstract\"] = paper_i[\"abstract\"]\n",
    "            else:\n",
    "                paper_i[\"abstract\"] = paper_j[\"abstract\"]\n",
    "                paper_i[\"abstract_ebds\"] = paper_j[\"abstract_ebds\"]\n",
    "                \n",
    "            paper_i[\"authors\"] = paper_i[\"authors\"] if len(paper_i[\"authors\"]) >= len(paper_j[\"authors\"]) else paper_j[\"authors\"]\n",
    "            \n",
    "            if paper_i[\"doi\"] == None and paper_j[\"doi\"] != None:\n",
    "                paper_i[\"doi\"] = paper_j[\"doi\"]\n",
    "                \n",
    "            paper_j[\"duplicated\"] = 1\n",
    "        else:\n",
    "            break\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"processing time: {end_time - start_time}s\")\n",
    "print(f\"avarage processing time: {(end_time - start_time) / len(combined_list_ebds)}\")\n",
    "print(f\"total item: {len(combined_list_ebds)}\")\n",
    "print(f\"total filtered item: {len(combined_list_ebds) - len(deleteList)}\")\n",
    "\n",
    "# 注1：后续可以通过paper[\"duplicated\"] == 0 获取unique paper。如下面代码所示：\n",
    "# for paper in combined_list_ebds:\n",
    "#     if paper[\"duplicated\"] == 0:\n",
    "#         print(paper[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f902ab8e-48ab-412f-80d6-215e27f39e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retained_list_ebds[0][\"title_ebds\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4ba387f8-24bb-4e14-9e68-1115515e5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果后处理：将unique的结果保存到本地\n",
    "retained_list_ebds = [paper for paper in combined_list_ebds if paper[\"duplicated\"] == 0]\n",
    "with open(\"./results/result_2_bert.pkl\",\"wb\") as fp:\n",
    "    pickle.dump(retained_list_ebds,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "a8a1ea7b-c04f-43e6-ba1f-5df02f55a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scopus + WoS = 354\n",
    "# Scopus + Wos + ASCE = 354 + 74 = 428"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5845b63b-90c8-4c8e-949e-32a813b51e9b",
   "metadata": {},
   "source": [
    "### 2.2 用OpenAI_Embedding进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1612daca-2406-40ba-851c-bac685e3da0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dotenv\n",
      "  Downloading dotenv-0.0.5.tar.gz (2.4 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [78 lines of output]\n",
      "  E:\\software_anaconda\\envs\\LLM\\lib\\site-packages\\setuptools\\__init__.py:94: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Requirements should be satisfied by a PEP 517 installer.\n",
      "          If you are using pip, you can try `pip install --use-pep517`.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    dist.fetch_build_eggs(dist.setup_requires)\n",
      "    error: subprocess-exited-with-error\n",
      "  \n",
      "    python setup.py egg_info did not run successfully.\n",
      "    exit code: 1\n",
      "  \n",
      "    [16 lines of output]\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 2, in <module>\n",
      "      File \"<pip-setuptools-caller>\", line 14, in <module>\n",
      "      File \"C:\\Users\\HKU-i5-Oscar\\AppData\\Local\\Temp\\pip-wheel-wbbrq7q9\\distribute_f804abba5c0347b1a9a29cf2e07bcd4e\\setuptools\\__init__.py\", line 2, in <module>\n",
      "        from setuptools.extension import Extension, Library\n",
      "      File \"C:\\Users\\HKU-i5-Oscar\\AppData\\Local\\Temp\\pip-wheel-wbbrq7q9\\distribute_f804abba5c0347b1a9a29cf2e07bcd4e\\setuptools\\extension.py\", line 5, in <module>\n",
      "        from setuptools.dist import _get_unpatched\n",
      "      File \"C:\\Users\\HKU-i5-Oscar\\AppData\\Local\\Temp\\pip-wheel-wbbrq7q9\\distribute_f804abba5c0347b1a9a29cf2e07bcd4e\\setuptools\\dist.py\", line 7, in <module>\n",
      "        from setuptools.command.install import install\n",
      "      File \"C:\\Users\\HKU-i5-Oscar\\AppData\\Local\\Temp\\pip-wheel-wbbrq7q9\\distribute_f804abba5c0347b1a9a29cf2e07bcd4e\\setuptools\\command\\__init__.py\", line 8, in <module>\n",
      "        from setuptools.command import install_scripts\n",
      "      File \"C:\\Users\\HKU-i5-Oscar\\AppData\\Local\\Temp\\pip-wheel-wbbrq7q9\\distribute_f804abba5c0347b1a9a29cf2e07bcd4e\\setuptools\\command\\install_scripts.py\", line 3, in <module>\n",
      "        from pkg_resources import Distribution, PathMetadata, ensure_directory\n",
      "      File \"C:\\Users\\HKU-i5-Oscar\\AppData\\Local\\Temp\\pip-wheel-wbbrq7q9\\distribute_f804abba5c0347b1a9a29cf2e07bcd4e\\pkg_resources.py\", line 1518, in <module>\n",
      "        register_loader_type(importlib_bootstrap.SourceFileLoader, DefaultProvider)\n",
      "    AttributeError: module 'importlib._bootstrap' has no attribute 'SourceFileLoader'\n",
      "    [end of output]\n",
      "  \n",
      "    note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  error: metadata-generation-failed\n",
      "  \n",
      "  Encountered error while generating package metadata.\n",
      "  \n",
      "  See above for output.\n",
      "  \n",
      "  note: This is an issue with the package mentioned above, not pip.\n",
      "  hint: See above for details.\n",
      "  Traceback (most recent call last):\n",
      "    File \"E:\\software_anaconda\\envs\\LLM\\lib\\site-packages\\setuptools\\installer.py\", line 102, in _fetch_build_egg_no_warn\n",
      "      subprocess.check_call(cmd)\n",
      "    File \"E:\\software_anaconda\\envs\\LLM\\lib\\subprocess.py\", line 373, in check_call\n",
      "      raise CalledProcessError(retcode, cmd)\n",
      "  subprocess.CalledProcessError: Command '['E:\\\\software_anaconda\\\\envs\\\\LLM\\\\python.exe', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', 'C:\\\\Users\\\\HKU-I5~1\\\\AppData\\\\Local\\\\Temp\\\\tmpqevwrzt8', '--quiet', 'distribute']' returned non-zero exit status 1.\n",
      "  \n",
      "  The above exception was the direct cause of the following exception:\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\HKU-i5-Oscar\\AppData\\Local\\Temp\\pip-install-94vs_6oh\\dotenv_3b94d26f91f04b4896cc1098efac5312\\setup.py\", line 13, in <module>\n",
      "      setup(name='dotenv',\n",
      "    File \"E:\\software_anaconda\\envs\\LLM\\lib\\site-packages\\setuptools\\__init__.py\", line 116, in setup\n",
      "      _install_setup_requires(attrs)\n",
      "    File \"E:\\software_anaconda\\envs\\LLM\\lib\\site-packages\\setuptools\\__init__.py\", line 89, in _install_setup_requires\n",
      "      _fetch_build_eggs(dist)\n",
      "    File \"E:\\software_anaconda\\envs\\LLM\\lib\\site-packages\\setuptools\\__init__.py\", line 94, in _fetch_build_eggs\n",
      "      dist.fetch_build_eggs(dist.setup_requires)\n",
      "    File \"E:\\software_anaconda\\envs\\LLM\\lib\\site-packages\\setuptools\\dist.py\", line 617, in fetch_build_eggs\n",
      "      return _fetch_build_eggs(self, requires)\n",
      "    File \"E:\\software_anaconda\\envs\\LLM\\lib\\site-packages\\setuptools\\installer.py\", line 39, in _fetch_build_eggs\n",
      "      resolved_dists = pkg_resources.working_set.resolve(\n",
      "    File \"E:\\software_anaconda\\envs\\LLM\\lib\\site-packages\\pkg_resources\\__init__.py\", line 897, in resolve\n",
      "      dist = self._resolve_dist(\n",
      "    File \"E:\\software_anaconda\\envs\\LLM\\lib\\site-packages\\pkg_resources\\__init__.py\", line 933, in _resolve_dist\n",
      "      dist = best[req.key] = env.best_match(\n",
      "    File \"E:\\software_anaconda\\envs\\LLM\\lib\\site-packages\\pkg_resources\\__init__.py\", line 1271, in best_match\n",
      "      return self.obtain(req, installer)\n",
      "    File \"E:\\software_anaconda\\envs\\LLM\\lib\\site-packages\\pkg_resources\\__init__.py\", line 1307, in obtain\n",
      "      return installer(requirement) if installer else None\n",
      "    File \"E:\\software_anaconda\\envs\\LLM\\lib\\site-packages\\setuptools\\installer.py\", line 104, in _fetch_build_egg_no_warn\n",
      "      raise DistutilsError(str(e)) from e\n",
      "  distutils.errors.DistutilsError: Command '['E:\\\\software_anaconda\\\\envs\\\\LLM\\\\python.exe', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', 'C:\\\\Users\\\\HKU-I5~1\\\\AppData\\\\Local\\\\Temp\\\\tmpqevwrzt8', '--quiet', 'distribute']' returned non-zero exit status 1.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45235c5f-f700-4cfa-a1a5-c64bd97c2dd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv, find_dotenv\n\u001b[0;32m      4\u001b[0m _ \u001b[38;5;241m=\u001b[39m load_dotenv(find_dotenv())\n\u001b[0;32m      5\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d83b5d44-f5d4-4d79-b463-2c1947537cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "92cd7400-0cc8-4806-9a38-746b88f9bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings_openai(sentence, model=\"text-embedding-3-small\"):\n",
    "    result = client.embeddings.create(\n",
    "        model = model, # 0.0020\n",
    "        input = sentence,\n",
    "        encoding_format=\"float\"\n",
    "    )\n",
    "    return np.array(result.data[0].embedding).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "359bc20e-7075-407f-b2ed-80b32ef3cce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04569226,  0.00586341,  0.06200783, ...,  0.02112128,\n",
       "        -0.01910353, -0.02716368]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getEmbeddings_openai(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "92f5cc74-2e74-4485-9f3e-d1efed87d9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 239/239 [04:34<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 274.1270213127136s\n",
      "average time: 1.1469749845720236s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wos_list_ebds = []\n",
    "wos_time_start = time.time()\n",
    "for i in tqdm(range(len(wos_list))):\n",
    "    paper = wos_list[i]\n",
    "    title = paper[\"title\"]\n",
    "    abstract = paper[\"abstract\"]\n",
    "    title_ebds = getEmbeddings_openai(title)\n",
    "    abstract_ebds = getEmbeddings_openai(abstract[:100])\n",
    "    paper[\"title_ebds\"] = title_ebds\n",
    "    paper[\"abstract_ebds\"] = abstract_ebds\n",
    "    paper[\"duplicated\"] = 0\n",
    "    wos_list_ebds.append(paper)\n",
    "wos_time_end = time.time()\n",
    "print(f\"total time: {wos_time_end - wos_time_start}s\")\n",
    "print(f\"average time: {(wos_time_end - wos_time_start)/len(wos_list)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fe4fbd8d-e8fc-4d0d-a4fb-5a654cad57f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 329/329 [06:08<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 368.5307791233063s\n",
      "average time: 1.1201543438398367s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scopus_list_ebds = []\n",
    "scopus_time_start = time.time()\n",
    "for i in tqdm(range(len(scopus_list))):\n",
    "    paper = scopus_list[i]\n",
    "    title = paper[\"title\"]\n",
    "    abstract = paper[\"abstract\"]\n",
    "    title_ebds = getEmbeddings_openai(title)\n",
    "    abstract_ebds = getEmbeddings_openai(abstract[:100])\n",
    "    paper[\"title_ebds\"] = title_ebds\n",
    "    paper[\"abstract_ebds\"] = abstract_ebds\n",
    "    paper[\"duplicated\"] = 0\n",
    "    scopus_list_ebds.append(paper)\n",
    "scopus_time_end = time.time()\n",
    "print(f\"total time: {scopus_time_end - scopus_time_start}s\")\n",
    "print(f\"average time: {(scopus_time_end - scopus_time_start)/len(scopus_list)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6ff4bac1-0852-43b2-8e37-9e34c64eca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 145/145 [02:40<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 160.12268996238708s\n",
      "average time: 1.104294413533704s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "asce_list_ebds = []\n",
    "asce_time_start = time.time()\n",
    "for i in tqdm(range(len(asce_list))):\n",
    "    paper = asce_list[i]\n",
    "    title = paper[\"title\"]\n",
    "    abstract = paper[\"abstract\"]\n",
    "    title_ebds = getEmbeddings_openai(title)\n",
    "    abstract_ebds = getEmbeddings_openai(abstract[:100])\n",
    "    paper[\"title_ebds\"] = title_ebds\n",
    "    paper[\"abstract_ebds\"] = abstract_ebds\n",
    "    paper[\"duplicated\"] = 0\n",
    "    asce_list_ebds.append(paper)\n",
    "asce_time_end = time.time()\n",
    "print(f\"total time: {asce_time_end - asce_time_start}s\")\n",
    "print(f\"average time: {(asce_time_end - asce_time_start)/len(asce_list)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "99307baa-ede1-471e-b854-cbbfa877b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_list_ebds_backup = copy.deepcopy(wos_list_ebds)\n",
    "scopus_list_ebds_backup = copy.deepcopy(scopus_list_ebds)\n",
    "asce_list_ebds_backup = copy.deepcopy(asce_list_ebds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "012b78ae-941b-4103-a64f-c123087bad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_list_ebds = wos_list_ebds_backup.copy()\n",
    "scopus_list_ebds = scopus_list_ebds_backup.copy()\n",
    "asce_list_ebds = asce_list_ebds_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fe10733b-2b77-4f06-bc02-79b1e8622e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_list_ebds = []\n",
    "combined_list_ebds.extend(wos_list_ebds)\n",
    "combined_list_ebds.extend(scopus_list_ebds)\n",
    "combined_list_ebds.extend(asce_list_ebds)\n",
    "combined_list_ebds = sorted(combined_list_ebds, key=lambda x:x[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ea4cef7e-4097-447b-a15b-caebd4051fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing time: 0.24077630043029785s\n",
      "avarage processing time: 0.0003376946710102354\n",
      "total item: 713\n",
      "total filtered item: 496\n"
     ]
    }
   ],
   "source": [
    "deleteList = []\n",
    "start_time = time.time()\n",
    "for i in range(0, len(combined_list_ebds)):\n",
    "    if i in deleteList:\n",
    "        continue\n",
    "    paper_i = combined_list_ebds[i]\n",
    "    paper_i_title_ebds = paper_i[\"title_ebds\"]\n",
    "\n",
    "    for j in range(i+1, len(combined_list_ebds)):\n",
    "        paper_j = combined_list_ebds[j]\n",
    "        paper_j_title_ebds = paper_j[\"title_ebds\"]\n",
    "        title_similarity = cosine_similarity(paper_i_title_ebds, paper_j_title_ebds)\n",
    "        if title_similarity > 0.95:\n",
    "            deleteList.append(j)\n",
    "            \n",
    "            if len(paper_i[\"abstract\"]) <= len(paper_j[\"abstract\"]):\n",
    "                paper_i[\"abstract\"] = paper_i[\"abstract\"]\n",
    "            else:\n",
    "                paper_i[\"abstract\"] = paper_j[\"abstract\"]\n",
    "                paper_i[\"abstract_ebds\"] = paper_j[\"abstract_ebds\"]\n",
    "                \n",
    "            paper_i[\"authors\"] = paper_i[\"authors\"] if len(paper_i[\"authors\"]) >= len(paper_j[\"authors\"]) else paper_j[\"authors\"]\n",
    "            \n",
    "            if paper_i[\"doi\"] == None and paper_j[\"doi\"] != None:\n",
    "                paper_i[\"doi\"] = paper_j[\"doi\"]\n",
    "                \n",
    "            paper_j[\"duplicated\"] = 1\n",
    "        else:\n",
    "            break\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"processing time: {end_time - start_time}s\")\n",
    "print(f\"avarage processing time: {(end_time - start_time) / len(combined_list_ebds)}\")\n",
    "print(f\"total item: {len(combined_list_ebds)}\")\n",
    "print(f\"total filtered item: {len(combined_list_ebds) - len(deleteList)}\")\n",
    "\n",
    "# 注1：后续可以通过paper[\"duplicated\"] == 0 获取unique paper。如下面代码所示：\n",
    "# for paper in combined_list_ebds:\n",
    "#     if paper[\"duplicated\"] == 0:\n",
    "#         print(paper[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ca37aefd-858d-47a7-b30d-c87e466935b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果后处理：将unique的结果保存到本地\n",
    "retained_list_ebds = [paper for paper in combined_list_ebds if paper[\"duplicated\"] == 0]\n",
    "with open(\"./results/result_2_openai.pkl\",\"wb\") as fp:\n",
    "    pickle.dump(retained_list_ebds,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4a6fdea0-7eba-4058-be33-35aa7036a8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retained_list_ebds[0][\"title_ebds\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202986bc-2021-469e-bc28-b48b82134b21",
   "metadata": {},
   "source": [
    "### 2.3 用Llama 3进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cbe5ca2a-6c7d-46a6-a0ca-92a0a25caef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "52cab463-1256-4b71-9464-5c70410baa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings_ollama(sentence, model=\"llama2\"):\n",
    "    outputs = ollama.embeddings(model=model, prompt=sentence)\n",
    "    return np.array(outputs[\"embedding\"]).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1062a783-803b-4c5d-9861-c9ddb15ba0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 239/239 [00:31<00:00,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 31.120624542236328s\n",
      "average time: 0.13021181816835284s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wos_list_ebds = []\n",
    "wos_time_start = time.time()\n",
    "for i in tqdm(range(len(wos_list))):\n",
    "    paper = wos_list[i]\n",
    "    title = paper[\"title\"]\n",
    "    abstract = paper[\"abstract\"]\n",
    "    title_ebds = getEmbeddings_ollama(title)\n",
    "    abstract_ebds = getEmbeddings_ollama(abstract[:100])\n",
    "    paper[\"title_ebds\"] = title_ebds\n",
    "    paper[\"abstract_ebds\"] = abstract_ebds\n",
    "    paper[\"duplicated\"] = 0\n",
    "    wos_list_ebds.append(paper)\n",
    "wos_time_end = time.time()\n",
    "print(f\"total time: {wos_time_end - wos_time_start}s\")\n",
    "print(f\"average time: {(wos_time_end - wos_time_start)/len(wos_list)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "52e76335-cca9-497e-aaa5-e9d26716a0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 329/329 [00:39<00:00,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 39.464702129364014s\n",
      "average time: 0.11995350191296053s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scopus_list_ebds = []\n",
    "scopus_time_start = time.time()\n",
    "for i in tqdm(range(len(scopus_list))):\n",
    "    paper = scopus_list[i]\n",
    "    title = paper[\"title\"]\n",
    "    abstract = paper[\"abstract\"]\n",
    "    title_ebds = getEmbeddings_ollama(title)\n",
    "    abstract_ebds = getEmbeddings_ollama(abstract[:100])\n",
    "    paper[\"title_ebds\"] = title_ebds\n",
    "    paper[\"abstract_ebds\"] = abstract_ebds\n",
    "    paper[\"duplicated\"] = 0\n",
    "    scopus_list_ebds.append(paper)\n",
    "scopus_time_end = time.time()\n",
    "print(f\"total time: {scopus_time_end - scopus_time_start}s\")\n",
    "print(f\"average time: {(scopus_time_end - scopus_time_start)/len(scopus_list)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c808822a-1581-4c0e-b02e-a30df4339240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 145/145 [00:17<00:00,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 17.375646829605103s\n",
      "average time: 0.11983204710072484s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "asce_list_ebds = []\n",
    "asce_time_start = time.time()\n",
    "for i in tqdm(range(len(asce_list))):\n",
    "    paper = asce_list[i]\n",
    "    title = paper[\"title\"]\n",
    "    abstract = paper[\"abstract\"]\n",
    "    title_ebds = getEmbeddings_ollama(title)\n",
    "    abstract_ebds = getEmbeddings_ollama(abstract[:100])\n",
    "    paper[\"title_ebds\"] = title_ebds\n",
    "    paper[\"abstract_ebds\"] = abstract_ebds\n",
    "    paper[\"duplicated\"] = 0\n",
    "    asce_list_ebds.append(paper)\n",
    "asce_time_end = time.time()\n",
    "print(f\"total time: {asce_time_end - asce_time_start}s\")\n",
    "print(f\"average time: {(asce_time_end - asce_time_start)/len(asce_list)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b6a861cf-bbf5-4b09-852c-4910c2ccc34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_list_ebds_backup = copy.deepcopy(wos_list_ebds)\n",
    "scopus_list_ebds_backup = copy.deepcopy(scopus_list_ebds)\n",
    "asce_list_ebds_backup = copy.deepcopy(asce_list_ebds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9720355c-c65f-4154-81a4-f068d5dc5096",
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_list_ebds = wos_list_ebds_backup.copy()\n",
    "scopus_list_ebds = scopus_list_ebds_backup.copy()\n",
    "asce_list_ebds = asce_list_ebds_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5bb55a32-e04f-449a-907a-7aea69d28003",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_list_ebds = []\n",
    "combined_list_ebds.extend(wos_list_ebds)\n",
    "combined_list_ebds.extend(scopus_list_ebds)\n",
    "combined_list_ebds.extend(asce_list_ebds)\n",
    "combined_list_ebds = sorted(combined_list_ebds, key=lambda x:x[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1915b90d-6822-474f-8974-5872decc6d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing time: 0.5686345100402832s\n",
      "avarage processing time: 0.0007975238569989946\n",
      "total item: 713\n",
      "total filtered item: 496\n"
     ]
    }
   ],
   "source": [
    "deleteList = []\n",
    "start_time = time.time()\n",
    "for i in range(0, len(combined_list_ebds)):\n",
    "    if i in deleteList:\n",
    "        continue\n",
    "    paper_i = combined_list_ebds[i]\n",
    "    paper_i_title_ebds = paper_i[\"title_ebds\"]\n",
    "\n",
    "    for j in range(i+1, len(combined_list_ebds)):\n",
    "        paper_j = combined_list_ebds[j]\n",
    "        paper_j_title_ebds = paper_j[\"title_ebds\"]\n",
    "        title_similarity = cosine_similarity(paper_i_title_ebds, paper_j_title_ebds)\n",
    "        if title_similarity > 0.95:\n",
    "            deleteList.append(j)\n",
    "            \n",
    "            if len(paper_i[\"abstract\"]) <= len(paper_j[\"abstract\"]):\n",
    "                paper_i[\"abstract\"] = paper_i[\"abstract\"]\n",
    "            else:\n",
    "                paper_i[\"abstract\"] = paper_j[\"abstract\"]\n",
    "                paper_i[\"abstract_ebds\"] = paper_j[\"abstract_ebds\"]\n",
    "                \n",
    "            paper_i[\"authors\"] = paper_i[\"authors\"] if len(paper_i[\"authors\"]) >= len(paper_j[\"authors\"]) else paper_j[\"authors\"]\n",
    "            \n",
    "            if paper_i[\"doi\"] == None and paper_j[\"doi\"] != None:\n",
    "                paper_i[\"doi\"] = paper_j[\"doi\"]\n",
    "                \n",
    "            paper_j[\"duplicated\"] = 1\n",
    "        else:\n",
    "            break\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"processing time: {end_time - start_time}s\")\n",
    "print(f\"avarage processing time: {(end_time - start_time) / len(combined_list_ebds)}\")\n",
    "print(f\"total item: {len(combined_list_ebds)}\")\n",
    "print(f\"total filtered item: {len(combined_list_ebds) - len(deleteList)}\")\n",
    "\n",
    "# 注1：后续可以通过paper[\"duplicated\"] == 0 获取unique paper。如下面代码所示：\n",
    "# for paper in combined_list_ebds:\n",
    "#     if paper[\"duplicated\"] == 0:\n",
    "#         print(paper[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6697d446-ebe5-4f36-ae14-00d59b623036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果后处理：将unique的结果保存到本地\n",
    "retained_list_ebds = [paper for paper in combined_list_ebds if paper[\"duplicated\"] == 0]\n",
    "with open(\"./results/result_2_llama.pkl\",\"wb\") as fp:\n",
    "    pickle.dump(retained_list_ebds,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "170dd9c9-f1df-4e0d-b877-e5fd0d5d9552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retained_list_ebds[0][\"title_ebds\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc2554e-e159-4635-ad26-64f686a50589",
   "metadata": {},
   "source": [
    "### 保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2213e04d-2f17-4b06-9eb1-7d184acb5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/result_2_bert.pkl\", \"rb\") as fp:\n",
    "    papers_info = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3edc28bc-3131-45af-85b6-8b4329a5f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = [\"title\", \"source\", \"abstract\", \"type\", \"authors\", \"year\", \"doi\"]\n",
    "columns = [\"year\", \"title\", \"abstract\"]\n",
    "papers_df = pd.DataFrame(columns=columns)\n",
    "for paper in papers_info:\n",
    "    # paperList = pd.DataFrame([{\n",
    "    #     \"title\":paper[\"title\"], \n",
    "    #     \"source\":paper[\"source\"], \n",
    "    #     \"abstract\":paper[\"abstract\"], \n",
    "    #     \"type\":paper[\"type\"], \n",
    "    #     \"authors\":paper[\"authors\"], \n",
    "    #     \"year\":paper[\"year\"], \n",
    "    #     \"doi\":paper[\"doi\"]\n",
    "    # }])\n",
    "    paperList = pd.DataFrame([{\n",
    "        \"year\":paper[\"year\"], \n",
    "        \"title\":paper[\"title\"], \n",
    "        \"abstract\":paper[\"abstract\"]\n",
    "    }])\n",
    "    papers_df = pd.concat([papers_df, paperList], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "088d6660-f713-4c7b-813f-32b48700e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df.to_csv(\"./results/results_2.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccdf446-1429-4fb5-be3a-222717f20b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
